{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0385556c",
   "metadata": {},
   "source": [
    "### Pruebas para el RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eeb4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09e473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Esta es la funcion sigmoide que usaremos\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dec4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d256405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"En esta función: Definimos los atributos de la clase: numero de neuronas por capas, bias y weights\"\"\"\n",
    "        self.num_layers = len(sizes)#numero de capas\n",
    "        self.sizes = sizes# numero de neuronas\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]# Llenamos un vector con bias aleatorios.\n",
    "        self.weights = [np.random.randn(y, x)# Tambien llenamos un vector de la matriz con los weights aleatorios.\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # se tratará de implementar el sgd con inercia. estos son los vectores que almacenan los valores con momentum\n",
    "        self.velocity_biases = [np.zeros_like(y) for y in self.biases]\n",
    "        self.velocity_weights = [np.zeros_like(x) for x in self.weights]\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Esta función nos da el valor de inicialización 'a' determinandolo a partir de la f. sigmoide.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "        \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch, eta, mu=0.9):#agregue el factor mu\n",
    "        \"\"\"Aqui definimos la actualizacion de los bias y los weights de los minibatchs\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]# Usamos dos listas para almacenar las sumas de los gradientes de cada minibatch. Son del mismo tamano que las de las bias y los weights.\n",
    "        for x, y in mini_batch:# En este ciclo usamos la funcion del BackPropagation definida mas abajo en la que calculamos los gradientes de C (la funcion de coste) con los bias y weights 'originales'.\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        '''voy a intentar comparar los tiempos en la SGD con y sin inercia (lo comentado es del d sgd simple)\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb#Cuando se calculan todos los gradientes del minibatch, se actualizan los bias y los weights con la resta del aprendizaje.\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        # aqui es donde se actualizarían los pesos y los bias con la incercia del sgd.'''\n",
    "        \"\"\" # Este es el del SGD+Incercia\n",
    "        self.velocity_weights = [mu * v - (eta / len(mini_batch)) * nw \n",
    "                                 for v, nw in zip(self.velocity_weights, nabla_w)] #ahora a los pesos se agrega el factor de momento (mu) y vamos actualizando el valor del peso\n",
    "        self.weights = [w + v for w, v in zip(self.weights, self.velocity_weights)]\n",
    "    \n",
    "        self.velocity_biases = [mu * v - (eta / len(mini_batch)) * nb for v, nb in zip(self.velocity_biases, nabla_b)]#lo mismo con los bias.\n",
    "        self.biases = [b + v for b, v in zip(self.biases, self.velocity_biases)]\n",
    "        \"\"\"\n",
    "        self.velocity_weights = [mu * vw + (1-mu) * nw**2#actualización d los weights y pesos con momentum\n",
    "                     for vw, nw in zip(self.vel_w, nabla_w)]\n",
    "        self.weights = [w - (eta / np.sqrt(vw + 1e-8)) * nw #sacamos la actualizacion de los phi\n",
    "                        for w, vw, nw in zip(self.weights, self.vel_w, nabla_w)]#usamos \\epsilon=1e-8 para evitar el cero\n",
    "        self.velocity_biases = [mu * vb + (1-mu) * nb**2\n",
    "                     for vb, nb in zip(self.vel_b, nabla_b)]\n",
    "        self.biases = [b - (eta / np.sqrt(vb + 1e-8)) * nb \n",
    "                      for b, vb, nb in zip(self.biases, self.vel_b, nabla_b)]\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Codigo del Algoritmo BackPropagation\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] # Creamos las listas con los bias y los weights determinados.\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        activation = x\n",
    "        activations = [x] # Esta será la lista con todas las activaciones de las capas de la red. El primer valor es x\n",
    "        zs = [] # esta es la lista (vacia por el momento) para las sumas de los pesos de cada capa\n",
    "        for b, w in zip(self.biases, self.weights): #Este ciclo es para cada cada capa de la red.\n",
    "            z = np.dot(w, activation)+b# CAlculamos la 'a' para sacar la suma ponderada\n",
    "            zs.append(z) # Vamos agregando cada valor a la lista\n",
    "            activation = sigmoid(z) # Se calcula el valor de la f sigmoide con la a calculada\n",
    "            activations.append(activation) # la agregamos a la lista\n",
    "        \n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])# una vez tenemos la lista de las activaciones, sacamos el error de la ultima capa.\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # guardamos los errores en las listas \"nablas\" de atras hacia adelante.\n",
    "\n",
    "        for l in range(2, self.num_layers): #ahora usamos los errores para calcular los gradientes de la ultima capa.\n",
    "            z = zs[-l] #empezamos desde la capa anterior\n",
    "            sp = sigmoid_prime(z) #definimos la derivada\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp #calculamos el error en esa capa usando del error de la capa siguiente\n",
    "            nabla_b[-l] = delta# usamos el error para calcular los gradientes de la capa y los vamos guardando en las listas de \"nablas\".\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)# al final las listas ya tiene los gradientes de cada capa de la red.\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"En esta función evaluamos los datos de prueba que clasifico la red.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)# en esta lista se encuentran los resultados de la clasificación. Buscamos el argumento maximo de la función feedforward para calcular la salida de la red.\n",
    "                        for (x, y) in test_data]# el proceso se hace en cada valor donde tenemos el resultado verdadero y el valor predicho. Esto se almacena en un vector de dos columnas.\n",
    "        return sum(int(x == y) for (x, y) in test_results)# Presentamos la lista de resultados\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\" Definimos la funcion de costo con la derivada respecto a las activaciones de salida.\"\"\"\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba18d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
